seed_everything: 333
trainer:
  gpus: 2
  gradient_clip_val: 1.0
  default_root_dir: debug-tmp
  # val_check_interval: 1.0
  max_steps: &max_steps 50000
  check_val_every_n_epoch: 2
  log_every_n_steps: 1
  num_sanity_val_steps: 0
  logger:
    - class_path: lightning_modules.loggers.patched_loggers.PatchedWandbLogger
      init_args:
        entity: niansong1996
        project: trace-codegen
        name: debug-tmp
        log_model: False
        save_code: True
        offline: False     
  callbacks:
    - class_path: pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint
      init_args:
        monitor: exec_acc
        mode: max
        filename: '{step}-{exec_acc:.4f}-{exec_rate:.4f}'
        save_top_k: 5
    - class_path: pytorch_lightning.callbacks.LearningRateMonitor
      init_args:
        logging_interval: step
    - class_path: pytorch_lightning.callbacks.progress.TQDMProgressBar
      init_args:
        refresh_rate: 1

  accelerator: gpu
  # replace_sampler_ddp: False
  # https://github.com/PyTorchLightning/pytorch-lightning/issues/8262
  strategy: deepspeed_stage_2
  # strategy: ddp_find_unused_parameters_false
  # precision: 16
  # accumulate_grad_batches: 4
model:
  class_path: lightning_modules.models.gpt_stmt_mml_model.GptStmtMmlModel
  init_args:
    transformer_model_name: &transformer EleutherAI/gpt-neo-2.7B
    max_gen_len: 256
    max_sampling_len: 100
    sampling_temp: 0.2
    on_policy_sample_num: 1
    on_policy_sample_temp: 0.8
    sampling_temp_at_k: 0.8
    # pass_at_k: 80
    # additional_pass_at_k: [5, 10, 20, 50]
    # eval_pass_at_k_every_n_epochs: 1
    # max_generation_batches: 50
    gradient_ckpt: true
    measure_dedup_metrics: false
    length_diff_tolerance: 0
    exclude_context_loss: false
    mle_lambda: 1.0
    mml_lambda: 0.0
    # beta_smoothing: 0.25
    # marg_set_size: 10
    # max_buffer_size: 10
    # eval_greedy_search: true
    # load_samples_file: /home/v-ansongni/Code/trace-codegen/mathqa_dedup_train_samples.jsonl
    # load_ckpt_file: /home/v-ansongni/Code/trace-codegen/amlt/mathqa-state-finetuning-125M-line-state-all-mask-ctx-len-1648/gpt-neo-mathqa-state-finetuning/lightning_logs/version_0/checkpoints/step=46175-exec_acc=0.6650-exec_rate=0.9956.ckpt
    # load_ckpt_file: /home/v-ansongni/Code/trace-codegen/amlt/mathqa-dedup-partial-mml-len_norm-low-temp/gpt-neo-mathqa-state-finetuning/lightning_logs/version_0/checkpoints/step=55755-exec_acc=0.1263-exec_rate=0.9727.ckpt
    # load_ckpt_file: /home/v-ansongni/Code/trace-codegen/amlt/mathqa-125M-state-skip-ts/gpt-neo-mathqa-state-finetuning/lightning_logs/version_0/checkpoints/step=49727-exec_acc=0.7559-exec_rate=0.9956.ckpt
    # load_ckpt_file: /home/v-ansongni/Code/trace-codegen/amlt/mathqa-finetune-gpt-neo-125M-pad-left/gpt-neo-mathqa-finetuning/lightning_logs/version_0/checkpoints/step=54044-exec_acc=0.7715-exec_rate=0.9893.ckpt
    optimizer:
      class_path: torch.optim.adamw.AdamW
      init_args: 
        lr: 1.0e-4
        # lr: 0.0
        betas: 
          - 0.9
          - 0.999
        eps: 1.0e-8
        weight_decay: 0.1
    lr_scheduler:
      name: linear
      init_args:
        num_warmup_steps: 100
        num_training_steps: *max_steps

data:
  class_path: lightning_modules.datasets.mathqa_line_reader.MathQAMmlDataModule
  init_args:
    transformer_model_name: *transformer
    batch_size: 2
    val_batch_size: 4
    train_file_path: data/mathqa/train_dedup.jsonl
    val_file_path: data/mathqa/val_dedup.jsonl
    # train_max_instances: 40
    # val_max_instances: 20
    # few_shot_n: 4