seed_everything: 333
trainer:
  # strategy: ddp_find_unused_parameters_false
  gpus: 2
  gradient_clip_val: 1.0
  default_root_dir: debug-tmp
  val_check_interval: 5000
  # logger:
  #   - class_path: lightning_modules.loggers.patched_loggers.PatchedCSVLogger
  #     init_args:
  #       # save_dir: logs/csv
  #       name: csv
  #       # version: 0
  #   - class_path: lightning_modules.loggers.patched_loggers.PatchedTensorBoardLogger
  #     init_args:
  #       # save_dir: logs/tb
  #       name: tb
  #       # version: 0
    # - class_path: lightning_modules.loggers.patched_loggers.PatchedNeptuneLogger
    #   init_args:
    #     project_name: ansong.ni/trace-codegen
  callbacks:
    # - class_path: pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint
    #   init_args:
    #     monitor: perplexity
    #     mode: min
    #     filename: '{step}-{perplexity:.2f}-{cell_edit_dist:.2f}'
    #     save_top_k: 5
    - class_path: pytorch_lightning.callbacks.progress.TQDMProgressBar
      init_args:
        refresh_rate: 1
  strategy: 
    class_path: pytorch_lightning.plugins.DeepSpeedPlugin
    init_args:
      stage: 3
      offload_optimizer: true
      offload_parameters: true
      # overlap_comm: true
      # allgather_bucket_size: 7e8
      # reduce_bucket_size: 7e8
      # pin_memory: true
      # contiguous_gradients: true
  precision: 16
  # accumulate_grad_batches: 4

  #  - class_path: lightning_modules.callbacks.save_prediction_callback.SavePredictionCallback
model:
  class_path: lightning_modules.models.gpt_seq2seq_model.GptSeq2SeqModel
  init_args:
    transformer_model_name: &transformer EleutherAI/gpt-j-6B
    max_gen_len: 100
    sampling_temp: 0.2
    gradient_ckpt: true
data:
  class_path: lightning_modules.datasets.mathqa_line_reader.MathQADataModule
  init_args:
    transformer_model_name: *transformer
    batch_size: 1
    val_batch_size: 1
    val_file_path: data/mathqa/val_python_with_states.jsonl
    # val_max_instances: 100
    few_shot_n: 4
optimizer:
  class_path: torch.optim.adamw.AdamW
  init_args: 
    lr: 1.0e-4
    betas: 
      - 0.9
      - 0.95
    eps: 1.0e-8
    weight_decay: 0.1
